<code_sketch>
  [overview]: to act as an abstraction over a variety of possible LLM 'services,' which might be network APIs like OpenAI or Anthropic or OpenRouter, or might be some method of invoking an LLM locallyâ€”perhaps through a shell command invoking Ollama.
  [target_lang]: Typescript
  [custom_hints]: {
      comments: terse but fairly complete
      styleInspiration: react core source code
  }


  <sketch>
      // general note: should be stateful on whether user has selected, OpenAI, Anthropic, or Ollama, in addition to the selected model. If the user wants to use Ollama we should start it up by using "ollama run \${ model_name }" so it's ready to go by inference time. Should use node's "child - process" system for Ollama interaction.

      class LLM {
          // insert: enum for LLM providers; choose appropriate name. Should include entries for OpenAI, and Anthropic
          // insert: BaseConfig, OpenAIConfig, AnthropicConfig, and OllamaConfig types
          // insert: an "InferenceResult" type, has an id, result, and indicates error vs success

          constructor() {
              // should build up an API keys map by reading from a .env file where we have an entry for OpenAI and one for Anthropic; ollama doesn't require a key
          }

          infer(prompt, systemPrompt, config: LLMConfig /*not sure how to handle this type-wise since the type depends on the selected provider.. maybe we should have something like LLMConfig<Provider> ?*/) {
              // the config param should have sensible defaults when user doesn't explicitly define something
          }

          selectLLM(provider, model)
                        getActiveInferences() // should return an array of Promise<InferenceResult> for each still-running inference
                        getErrorInferences()
                        removeErrorInference(id)
                        cancelInference(id)
                        restartInference(id)
                    }
  </sketch>
</code_sketch>
